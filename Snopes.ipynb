{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Snopes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scl5MSa8rkk8"
      },
      "source": [
        "## **Installing Conda**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJN-MjTdrdPw"
      },
      "source": [
        "!which python # should return /usr/local/bin/python\n",
        "!python --version\n",
        "!echo $PYTHONPATH\n",
        "%env PYTHONPATH="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMEQr_BOrnaQ"
      },
      "source": [
        "%%bash\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfeIqvjrq4u"
      },
      "source": [
        "!which conda # should return /usr/local/bin/conda\n",
        "!conda --version # should return 4.5.4\n",
        "!which python # still returns /usr/local/bin/python\n",
        "!python --version # now returns Python 3.6.5 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StMOuSWXrsUe"
      },
      "source": [
        "%%bash\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEjgBpSZruAt"
      },
      "source": [
        "!conda --version # now returns 4.8.3\n",
        "!python --version # now returns Python 3.6.10 :: Anaconda, Inc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xdpRrBIrvV0"
      },
      "source": [
        "import sys\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uum-y0XDrw0s"
      },
      "source": [
        "!conda install --channel conda-forge featuretools --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL99cqpbrzTC"
      },
      "source": [
        "## **Installing Scrapy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK8MG8R2dNmY"
      },
      "source": [
        "!cd content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-9b6nDYtZ21"
      },
      "source": [
        "!conda install -c conda-forge scrapy\n",
        "!pip install Scrapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krLVLEOIr4L7"
      },
      "source": [
        "#### **Note:** Below is the spider script which needs to be written in `group7_spider.py` **NOT HERE**!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pPy4INxr7J7"
      },
      "source": [
        "import scrapy\n",
        "import json\n",
        "import re\n",
        "\n",
        "\n",
        "class FactchecksSpider(scrapy.Spider):\n",
        "    name = \"FactChecks\"\n",
        "    allowed_domains = [\"www.snopes.com\"]\n",
        "    start_urls = [\"https://www.snopes.com/fact-check/\"]\n",
        "    url_set = set({\"https://www.snopes.com/fact-check/\"})\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:48.0) Gecko/20100101 Firefox/48.0\"\n",
        "    }\n",
        "\n",
        "    def parse(self, response):\n",
        "        \"\"\"\n",
        "        This method crawls the pages and extracts:\n",
        "        1. links to the articles\n",
        "        2. link to next page\n",
        "        \"\"\"\n",
        "        links = response.css(\n",
        "            \"body > div.theme-content > div > div > main > div > div.media-list > article.media-wrapper > a::attr(href)\"\n",
        "        ).extract()\n",
        "        next_page = response.css(\".btn-next::attr(href)\").extract()\n",
        "        for link in links:\n",
        "            link = response.urljoin(link)\n",
        "            if link not in self.url_set:\n",
        "                self.url_set.add(link)\n",
        "                yield scrapy.Request(\n",
        "                    url=link, headers=self.headers, callback=self.parse_fact_details\n",
        "                )\n",
        "        if next_page:\n",
        "            if next_page[0] not in self.url_set:\n",
        "                self.url_set.add(next_page[0])\n",
        "                yield scrapy.Request(\n",
        "                    url=next_page[0], headers=self.headers, callback=self.parse\n",
        "                )\n",
        "        pass\n",
        "\n",
        "    def parse_fact_details(self, response):\n",
        "        \"\"\"This method crawls and extracts the details of each article.\"\"\"\n",
        "\n",
        "        links_in_content = response.css(\"div.content:nth-child(2) a::attr(href)\").extract()\n",
        "        for link in links_in_content:\n",
        "            if link.startswith(\"https://www.snopes.com/fact-check/\") and (link not in self.url_set):\n",
        "                self.url_set.add(link)\n",
        "                yield scrapy.Request(\n",
        "                    url=link, headers=FactchecksSpider.headers, callback=self.parse_fact_details\n",
        "                )\n",
        "\n",
        "        claim_list = response.css(\".claim > p:nth-child(1) ::text\").extract()\n",
        "        claim = ''\n",
        "        if len(claim_list):\n",
        "            claim = claim_list[0]\n",
        "\n",
        "        content = ''\n",
        "        content_body_list = response.css(\"div.content:nth-child(2)\").extract()\n",
        "        if len(content_body_list):\n",
        "            content_body = content_body_list[0]\n",
        "            content = self.remove_html_tags(content_body)\n",
        "\n",
        "        pattern = r'\\bvar\\s+snopesPageData\\s*=\\s*(\\{.*?\\})\\s*;\\s*\\n'\n",
        "        json_data = response.css(\"script::text\").re_first(pattern)\n",
        "        json_res = json.loads(json_data)\n",
        "        title_list = response.css(\"h1.title::text\").extract()\n",
        "        title = title_list[0]\n",
        "        item = FactCheckItem()\n",
        "\n",
        "        if title:\n",
        "            item[\"title\"] = title\n",
        "        if \"url\" in json_res:\n",
        "            item[\"url\"] = json_res[\"url\"]\n",
        "        if \"date_published\" in json_res:\n",
        "            item[\"date_published\"] = json_res[\"date_published\"]\n",
        "        if \"rating\" in json_res:\n",
        "            item[\"rating\"] = json_res[\"rating\"]\n",
        "        if \"author_name\" in json_res:\n",
        "            item[\"author_name\"] = json_res[\"author_name\"]\n",
        "        if \"category\" in json_res:\n",
        "            item[\"category\"] = json_res[\"category\"]\n",
        "        if \"tags\" in json_res:\n",
        "            item[\"tags\"] = json_res[\"tags\"]\n",
        "        item[\"claim\"] = claim\n",
        "        item[\"content\"] = content\n",
        "        yield item\n",
        "        pass\n",
        "\n",
        "    def remove_html_tags(self, text):\n",
        "        \"\"\"This method removes HTML tags from a string\"\"\"\n",
        "        clean_script = re.compile('<script[^>]*>[\\s\\S​]*?</script>')\n",
        "        clean_image_caption = re.compile('<figcaption[^>]*>[\\s\\S​]*?</figcaption>')\n",
        "        clean_iframe = re.compile('<iframe[^>]*>[\\s\\S​]*?</iframe>')\n",
        "        clean_all_tags = re.compile('<.*?>')\n",
        "\n",
        "        # remove script tag with its content\n",
        "        text = re.sub(clean_script, '', text)\n",
        "        # remove caption of an image with its content\n",
        "        text = re.sub(clean_image_caption, '', text)\n",
        "        text = re.sub(clean_iframe, '', text)\n",
        "        text = re.sub(\"[\\n]+\", \" \", text)\n",
        "        text = re.sub(\"[\\t]+\", \"\", text)\n",
        "        # \\xa0 is actually non-breaking space in Latin1 (ISO 8859-1), also chr(160).\n",
        "        # You should replace it with a space.\n",
        "        text = re.sub(\"[\\xa0]+\", \" \", text)\n",
        "        return re.sub(clean_all_tags, '', text)\n",
        "\n",
        "\n",
        "    # def get_content(self, response):\n",
        "    #     body = response.css(\"div.content:nth-child(2)\").extract()\n",
        "    #     links_in_content = response.css(\n",
        "    #         \"div.content:nth-child(2) a::attr(href)\"\n",
        "    #     ).extract()\n",
        "    #     content = \"\"\n",
        "    #     for link in links_in_content:\n",
        "    #         if link.startswith(\"https://www.snopes.com/fact-check/\") and (\n",
        "    #                 link not in self.url_set\n",
        "    #         ):\n",
        "    #             self.url_set.add(link)\n",
        "    #             yield scrapy.Request(\n",
        "    #                 url=link, headers=FactchecksSpider.headers, callback=self.parse\n",
        "    #             )\n",
        "    #\n",
        "    #     claim = response.css(\".claim > p:nth-child(1) ::text\").extract()\n",
        "    #\n",
        "    #     if claim[0]:\n",
        "    #         content = claim[0]\n",
        "    #     content  += self.get_content_of_p_tag(response)\n",
        "    #     return content\n",
        "    #\n",
        "    # def get_content_of_p_tag(self, response):\n",
        "    #     counter_p_tags = 1\n",
        "    #     number_of_p_tags = len(response.xpath('/html/body/div[4]/div/div/main/article/div[7]/div[1]/p'.format(counter_p_tags)).extract())\n",
        "    #     content = ''\n",
        "    #     p_body = response.xpath('/html/body/div[4]/div/div/main/article/div[7]/div[1]/p[{}]/text()'.format(counter_p_tags)).extract()\n",
        "    #     p_text_with_tag = response.xpath('/html/body/div[4]/div/div/main/article/div[7]/div[1]/p[{}]'.format(counter_p_tags)).extract()\n",
        "    #     while counter_p_tags <= number_of_p_tags:\n",
        "    #         all_p_text = \"\"\n",
        "    #         p_str = \"\"\n",
        "    #         if p_body:\n",
        "    #             selector_list = Selector(text=p_text_with_tag[0]).xpath('//a/text()').extract()  # get all text of a tags in tag p\n",
        "    #             length = len(selector_list)\n",
        "    #             selector_list_index = 0\n",
        "    #             for p in p_body:\n",
        "    #                 p_str += p\n",
        "    #                 if selector_list_index < length:\n",
        "    #                     p_str += selector_list[selector_list_index]\n",
        "    #                     selector_list_index += 1\n",
        "    #             all_p_text = p_str\n",
        "    #         content += 'p[{}]'.format(counter_p_tags) + all_p_text\n",
        "    #         counter_p_tags += 1\n",
        "    #         p_body = response.xpath('/html/body/div[4]/div/div/main/article/div[7]/div[1]/p[{}]/text()'.format(counter_p_tags)).extract()\n",
        "    #         p_text_with_tag = response.xpath('/html/body/div[4]/div/div/main/article/div[7]/div[1]/p[{}]'.format(counter_p_tags)).extract()\n",
        "    #\n",
        "    #     return content\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5AhjVVlt4v1"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWIBRUOcsQgP"
      },
      "source": [
        "## **Run the Spider**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqigANxYsQPM"
      },
      "source": [
        "!pwd it should be /content/drive/MyDrive/exercise4/snopes/snopes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdo0qW90FdZL"
      },
      "source": [
        "%cd /content/drive/MyDrive/exercise4/snopes/snopes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbTwfmjoVp7h"
      },
      "source": [
        "!scrapy crawl Factchecks -o result.csv "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsW4FtDlMFSq"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('./result.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2DsCMOwdW_4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}